{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# uggh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## bigger uggh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import functools\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_names = [\n",
    "    'elevation',\n",
    "    'aspect',\n",
    "    'slope',\n",
    "    'h_dist_hydro',\n",
    "    'v_dist_hydro',\n",
    "    'h_dist_road',\n",
    "    'shade_9am',\n",
    "    'shade_noon',\n",
    "    'shade_3pm',\n",
    "    'h_dist_fire'\n",
    "]\n",
    "\n",
    "col_names += ['wild_' + str(i) for i in range(4)]\n",
    "col_names += ['soil_' + str(i) for i in range(40)]\n",
    "col_names.append('cover_type')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "with gzip.open('file.txt.gz', 'rb') as f_in, open('file.txt', 'wb') as f_out:\n",
    "    shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('covtype.data.gz', names = col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of         elevation  aspect  slope  h_dist_hydro  v_dist_hydro  h_dist_road  \\\n",
       "0            2596      51      3           258             0          510   \n",
       "1            2590      56      2           212            -6          390   \n",
       "2            2804     139      9           268            65         3180   \n",
       "3            2785     155     18           242           118         3090   \n",
       "4            2595      45      2           153            -1          391   \n",
       "5            2579     132      6           300           -15           67   \n",
       "6            2606      45      7           270             5          633   \n",
       "7            2605      49      4           234             7          573   \n",
       "8            2617      45      9           240            56          666   \n",
       "9            2612      59     10           247            11          636   \n",
       "10           2612     201      4           180            51          735   \n",
       "11           2886     151     11           371            26         5253   \n",
       "12           2742     134     22           150            69         3215   \n",
       "13           2609     214      7           150            46          771   \n",
       "14           2503     157      4            67             4          674   \n",
       "15           2495      51      7            42             2          752   \n",
       "16           2610     259      1           120            -1          607   \n",
       "17           2517      72      7            85             6          595   \n",
       "18           2504       0      4            95             5          691   \n",
       "19           2503      38      5            85            10          741   \n",
       "20           2501      71      9            60             8          767   \n",
       "21           2880     209     17           216            30         4986   \n",
       "22           2768     114     23           192            82         3339   \n",
       "23           2511      54      8           124             0          638   \n",
       "24           2507      22      9           120            14          732   \n",
       "25           2492     135      6             0             0          860   \n",
       "26           2489     163     10            30            -4          849   \n",
       "27           2962     148     16           323            23         5916   \n",
       "28           2811     135      1           212            30         3670   \n",
       "29           2739     117     24           127            53         3281   \n",
       "...           ...     ...    ...           ...           ...          ...   \n",
       "580982       2431     182      9           300            23          268   \n",
       "580983       2436     219     15           300            28          256   \n",
       "580984       2441     201     15           300            33          247   \n",
       "580985       2442     191     17           300            34          242   \n",
       "580986       2444     202     21           300            37          240   \n",
       "580987       2450     203     27           300            53          240   \n",
       "580988       2455     189     31           295            58          240   \n",
       "580989       2455     181     34           277            58          240   \n",
       "580990       2455     166     35           258            58          240   \n",
       "580991       2445     157     33           242            49          240   \n",
       "580992       2441     173     28           228            45          242   \n",
       "580993       2440     173     26           216            44          234   \n",
       "580994       2437     170     23           201            45          216   \n",
       "580995       2437     174     22           190            45          201   \n",
       "580996       2435     171     22           175            43          190   \n",
       "580997       2433     168     23           162            41          175   \n",
       "580998       2429     166     24           153            45          162   \n",
       "580999       2426     168     24           150            42          153   \n",
       "581000       2423     169     24           134            39          150   \n",
       "581001       2421     172     25           124            35          134   \n",
       "581002       2419     168     25           108            33          124   \n",
       "581003       2415     161     25            95            29          120   \n",
       "581004       2410     158     24            90            24          120   \n",
       "581005       2405     159     22            90            19          120   \n",
       "581006       2401     157     21            90            15          120   \n",
       "581007       2396     153     20            85            17          108   \n",
       "581008       2391     152     19            67            12           95   \n",
       "581009       2386     159     17            60             7           90   \n",
       "581010       2384     170     15            60             5           90   \n",
       "581011       2383     165     13            60             4           67   \n",
       "\n",
       "        shade_9am  shade_noon  shade_3pm  h_dist_fire     ...      soil_31  \\\n",
       "0             221         232        148         6279     ...            0   \n",
       "1             220         235        151         6225     ...            0   \n",
       "2             234         238        135         6121     ...            0   \n",
       "3             238         238        122         6211     ...            0   \n",
       "4             220         234        150         6172     ...            0   \n",
       "5             230         237        140         6031     ...            0   \n",
       "6             222         225        138         6256     ...            0   \n",
       "7             222         230        144         6228     ...            0   \n",
       "8             223         221        133         6244     ...            0   \n",
       "9             228         219        124         6230     ...            0   \n",
       "10            218         243        161         6222     ...            0   \n",
       "11            234         240        136         4051     ...            0   \n",
       "12            248         224         92         6091     ...            0   \n",
       "13            213         247        170         6211     ...            0   \n",
       "14            224         240        151         5600     ...            0   \n",
       "15            224         225        137         5576     ...            0   \n",
       "16            216         239        161         6096     ...            0   \n",
       "17            228         227        133         5607     ...            0   \n",
       "18            214         232        156         5572     ...            0   \n",
       "19            220         228        144         5555     ...            0   \n",
       "20            230         223        126         5547     ...            0   \n",
       "21            206         253        179         4323     ...            0   \n",
       "22            252         209         71         5972     ...            0   \n",
       "23            225         222        130         5569     ...            0   \n",
       "24            215         221        143         5534     ...            0   \n",
       "25            229         237        142         5494     ...            0   \n",
       "26            230         243        145         5486     ...            0   \n",
       "27            240         236        120         3395     ...            0   \n",
       "28            220         238        154         5643     ...            0   \n",
       "29            253         210         71         6033     ...            0   \n",
       "...           ...         ...        ...          ...     ...          ...   \n",
       "580982        223         246        156          973     ...            0   \n",
       "580983        203         253        184          957     ...            0   \n",
       "580984        212         252        171          942     ...            0   \n",
       "580985        217         251        162          927     ...            0   \n",
       "580986        205         253        174          912     ...            0   \n",
       "580987        196         252        176          899     ...            0   \n",
       "580988        205         245        152          886     ...            0   \n",
       "580989        210         238        133          875     ...            0   \n",
       "580990        225         227        103          864     ...            0   \n",
       "580991        235         224         91          854     ...            0   \n",
       "580992        225         240        128          845     ...            0   \n",
       "580993        226         242        132          837     ...            0   \n",
       "580994        229         242        131          830     ...            0   \n",
       "580995        227         245        139          824     ...            0   \n",
       "580996        229         244        135          819     ...            0   \n",
       "580997        231         241        128          815     ...            0   \n",
       "580998        232         240        125          812     ...            0   \n",
       "580999        231         241        127          811     ...            0   \n",
       "581000        230         241        128          810     ...            0   \n",
       "581001        227         242        132          811     ...            0   \n",
       "581002        230         240        126          812     ...            0   \n",
       "581003        236         237        116          815     ...            0   \n",
       "581004        238         236        115          819     ...            0   \n",
       "581005        237         238        119          824     ...            0   \n",
       "581006        238         238        119          830     ...            0   \n",
       "581007        240         237        118          837     ...            0   \n",
       "581008        240         237        119          845     ...            0   \n",
       "581009        236         241        130          854     ...            0   \n",
       "581010        230         245        143          864     ...            0   \n",
       "581011        231         244        141          875     ...            0   \n",
       "\n",
       "        soil_32  soil_33  soil_34  soil_35  soil_36  soil_37  soil_38  \\\n",
       "0             0        0        0        0        0        0        0   \n",
       "1             0        0        0        0        0        0        0   \n",
       "2             0        0        0        0        0        0        0   \n",
       "3             0        0        0        0        0        0        0   \n",
       "4             0        0        0        0        0        0        0   \n",
       "5             0        0        0        0        0        0        0   \n",
       "6             0        0        0        0        0        0        0   \n",
       "7             0        0        0        0        0        0        0   \n",
       "8             0        0        0        0        0        0        0   \n",
       "9             0        0        0        0        0        0        0   \n",
       "10            0        0        0        0        0        0        0   \n",
       "11            0        0        0        0        0        0        0   \n",
       "12            0        0        0        0        0        0        0   \n",
       "13            0        0        0        0        0        0        0   \n",
       "14            0        0        0        0        0        0        0   \n",
       "15            0        0        0        0        0        0        0   \n",
       "16            0        0        0        0        0        0        0   \n",
       "17            0        0        0        0        0        0        0   \n",
       "18            0        0        0        0        0        0        0   \n",
       "19            0        0        0        0        0        0        0   \n",
       "20            0        0        0        0        0        0        0   \n",
       "21            0        0        0        0        0        0        0   \n",
       "22            0        0        0        0        0        0        0   \n",
       "23            0        0        0        0        0        0        0   \n",
       "24            0        0        0        0        0        0        0   \n",
       "25            0        0        0        0        0        0        0   \n",
       "26            0        0        0        0        0        0        0   \n",
       "27            0        0        0        0        0        0        0   \n",
       "28            0        0        0        0        0        0        0   \n",
       "29            0        0        0        0        0        0        0   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "580982        0        0        0        0        0        0        0   \n",
       "580983        0        0        0        0        0        0        0   \n",
       "580984        0        0        0        0        0        0        0   \n",
       "580985        0        0        0        0        0        0        0   \n",
       "580986        0        0        0        0        0        0        0   \n",
       "580987        0        0        0        0        0        0        0   \n",
       "580988        0        0        0        0        0        0        0   \n",
       "580989        0        0        0        0        0        0        0   \n",
       "580990        0        0        0        0        0        0        0   \n",
       "580991        0        0        0        0        0        0        0   \n",
       "580992        0        0        0        0        0        0        0   \n",
       "580993        0        0        0        0        0        0        0   \n",
       "580994        0        0        0        0        0        0        0   \n",
       "580995        0        0        0        0        0        0        0   \n",
       "580996        0        0        0        0        0        0        0   \n",
       "580997        0        0        0        0        0        0        0   \n",
       "580998        0        0        0        0        0        0        0   \n",
       "580999        0        0        0        0        0        0        0   \n",
       "581000        0        0        0        0        0        0        0   \n",
       "581001        0        0        0        0        0        0        0   \n",
       "581002        0        0        0        0        0        0        0   \n",
       "581003        0        0        0        0        0        0        0   \n",
       "581004        0        0        0        0        0        0        0   \n",
       "581005        0        0        0        0        0        0        0   \n",
       "581006        0        0        0        0        0        0        0   \n",
       "581007        0        0        0        0        0        0        0   \n",
       "581008        0        0        0        0        0        0        0   \n",
       "581009        0        0        0        0        0        0        0   \n",
       "581010        0        0        0        0        0        0        0   \n",
       "581011        0        0        0        0        0        0        0   \n",
       "\n",
       "        soil_39  cover_type  \n",
       "0             0           5  \n",
       "1             0           5  \n",
       "2             0           2  \n",
       "3             0           2  \n",
       "4             0           5  \n",
       "5             0           2  \n",
       "6             0           5  \n",
       "7             0           5  \n",
       "8             0           5  \n",
       "9             0           5  \n",
       "10            0           5  \n",
       "11            0           2  \n",
       "12            0           2  \n",
       "13            0           5  \n",
       "14            0           5  \n",
       "15            0           5  \n",
       "16            0           5  \n",
       "17            0           5  \n",
       "18            0           5  \n",
       "19            0           5  \n",
       "20            0           5  \n",
       "21            0           2  \n",
       "22            0           5  \n",
       "23            0           5  \n",
       "24            0           5  \n",
       "25            0           5  \n",
       "26            0           5  \n",
       "27            0           2  \n",
       "28            0           2  \n",
       "29            0           5  \n",
       "...         ...         ...  \n",
       "580982        0           3  \n",
       "580983        0           3  \n",
       "580984        0           3  \n",
       "580985        0           3  \n",
       "580986        0           3  \n",
       "580987        0           3  \n",
       "580988        0           3  \n",
       "580989        0           3  \n",
       "580990        0           3  \n",
       "580991        0           3  \n",
       "580992        0           3  \n",
       "580993        0           3  \n",
       "580994        0           3  \n",
       "580995        0           3  \n",
       "580996        0           3  \n",
       "580997        0           3  \n",
       "580998        0           3  \n",
       "580999        0           3  \n",
       "581000        0           3  \n",
       "581001        0           3  \n",
       "581002        0           3  \n",
       "581003        0           3  \n",
       "581004        0           3  \n",
       "581005        0           3  \n",
       "581006        0           3  \n",
       "581007        0           3  \n",
       "581008        0           3  \n",
       "581009        0           3  \n",
       "581010        0           3  \n",
       "581011        0           3  \n",
       "\n",
       "[581012 rows x 55 columns]>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>elevation</th>\n",
       "      <th>aspect</th>\n",
       "      <th>slope</th>\n",
       "      <th>h_dist_hydro</th>\n",
       "      <th>v_dist_hydro</th>\n",
       "      <th>h_dist_road</th>\n",
       "      <th>shade_9am</th>\n",
       "      <th>shade_noon</th>\n",
       "      <th>shade_3pm</th>\n",
       "      <th>h_dist_fire</th>\n",
       "      <th>...</th>\n",
       "      <th>soil_31</th>\n",
       "      <th>soil_32</th>\n",
       "      <th>soil_33</th>\n",
       "      <th>soil_34</th>\n",
       "      <th>soil_35</th>\n",
       "      <th>soil_36</th>\n",
       "      <th>soil_37</th>\n",
       "      <th>soil_38</th>\n",
       "      <th>soil_39</th>\n",
       "      <th>cover_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "      <td>581012.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2959.365301</td>\n",
       "      <td>155.656807</td>\n",
       "      <td>14.103704</td>\n",
       "      <td>269.428217</td>\n",
       "      <td>46.418855</td>\n",
       "      <td>2350.146611</td>\n",
       "      <td>212.146049</td>\n",
       "      <td>223.318716</td>\n",
       "      <td>142.528263</td>\n",
       "      <td>1980.291226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090392</td>\n",
       "      <td>0.077716</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.026803</td>\n",
       "      <td>0.023762</td>\n",
       "      <td>0.015060</td>\n",
       "      <td>2.051471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>279.984734</td>\n",
       "      <td>111.913721</td>\n",
       "      <td>7.488242</td>\n",
       "      <td>212.549356</td>\n",
       "      <td>58.295232</td>\n",
       "      <td>1559.254870</td>\n",
       "      <td>26.769889</td>\n",
       "      <td>19.768697</td>\n",
       "      <td>38.274529</td>\n",
       "      <td>1324.195210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286743</td>\n",
       "      <td>0.267725</td>\n",
       "      <td>0.052584</td>\n",
       "      <td>0.056957</td>\n",
       "      <td>0.014310</td>\n",
       "      <td>0.022641</td>\n",
       "      <td>0.161508</td>\n",
       "      <td>0.152307</td>\n",
       "      <td>0.121791</td>\n",
       "      <td>1.396504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1859.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-173.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2809.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1106.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>213.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>1024.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2996.000000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1997.000000</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>1710.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3163.000000</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>384.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>3328.000000</td>\n",
       "      <td>231.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>2550.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3858.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>1397.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>7117.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>7173.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           elevation         aspect          slope   h_dist_hydro  \\\n",
       "count  581012.000000  581012.000000  581012.000000  581012.000000   \n",
       "mean     2959.365301     155.656807      14.103704     269.428217   \n",
       "std       279.984734     111.913721       7.488242     212.549356   \n",
       "min      1859.000000       0.000000       0.000000       0.000000   \n",
       "25%      2809.000000      58.000000       9.000000     108.000000   \n",
       "50%      2996.000000     127.000000      13.000000     218.000000   \n",
       "75%      3163.000000     260.000000      18.000000     384.000000   \n",
       "max      3858.000000     360.000000      66.000000    1397.000000   \n",
       "\n",
       "        v_dist_hydro    h_dist_road      shade_9am     shade_noon  \\\n",
       "count  581012.000000  581012.000000  581012.000000  581012.000000   \n",
       "mean       46.418855    2350.146611     212.146049     223.318716   \n",
       "std        58.295232    1559.254870      26.769889      19.768697   \n",
       "min      -173.000000       0.000000       0.000000       0.000000   \n",
       "25%         7.000000    1106.000000     198.000000     213.000000   \n",
       "50%        30.000000    1997.000000     218.000000     226.000000   \n",
       "75%        69.000000    3328.000000     231.000000     237.000000   \n",
       "max       601.000000    7117.000000     254.000000     254.000000   \n",
       "\n",
       "           shade_3pm    h_dist_fire      ...              soil_31  \\\n",
       "count  581012.000000  581012.000000      ...        581012.000000   \n",
       "mean      142.528263    1980.291226      ...             0.090392   \n",
       "std        38.274529    1324.195210      ...             0.286743   \n",
       "min         0.000000       0.000000      ...             0.000000   \n",
       "25%       119.000000    1024.000000      ...             0.000000   \n",
       "50%       143.000000    1710.000000      ...             0.000000   \n",
       "75%       168.000000    2550.000000      ...             0.000000   \n",
       "max       254.000000    7173.000000      ...             1.000000   \n",
       "\n",
       "             soil_32        soil_33        soil_34        soil_35  \\\n",
       "count  581012.000000  581012.000000  581012.000000  581012.000000   \n",
       "mean        0.077716       0.002773       0.003255       0.000205   \n",
       "std         0.267725       0.052584       0.056957       0.014310   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "             soil_36        soil_37        soil_38        soil_39  \\\n",
       "count  581012.000000  581012.000000  581012.000000  581012.000000   \n",
       "mean        0.000513       0.026803       0.023762       0.015060   \n",
       "std         0.022641       0.161508       0.152307       0.121791   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "          cover_type  \n",
       "count  581012.000000  \n",
       "mean        2.051471  \n",
       "std         1.396504  \n",
       "min         1.000000  \n",
       "25%         1.000000  \n",
       "50%         2.000000  \n",
       "75%         2.000000  \n",
       "max         7.000000  \n",
       "\n",
       "[8 rows x 55 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get X and Y\n",
    "X = df.values[:-7]\n",
    "Y = df.values[:,-7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.25466600e-03,   2.04815047e-04,   5.12898185e-04,\n",
       "         2.68032330e-02,   2.37619877e-02,   1.50599299e-02,\n",
       "         2.05147054e+00])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>581005.000000</td>\n",
       "      <td>581005.000000</td>\n",
       "      <td>581005.000000</td>\n",
       "      <td>581005.000000</td>\n",
       "      <td>581005.000000</td>\n",
       "      <td>581005.000000</td>\n",
       "      <td>581005.000000</td>\n",
       "      <td>581005.000000</td>\n",
       "      <td>581005.000000</td>\n",
       "      <td>581005.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>581005.000000</td>\n",
       "      <td>581005.000000</td>\n",
       "      <td>581005.000000</td>\n",
       "      <td>581005.000000</td>\n",
       "      <td>581005.000000</td>\n",
       "      <td>581005.000000</td>\n",
       "      <td>581005.000000</td>\n",
       "      <td>581005.000000</td>\n",
       "      <td>581005.000000</td>\n",
       "      <td>581005.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.550458</td>\n",
       "      <td>0.432380</td>\n",
       "      <td>0.213692</td>\n",
       "      <td>0.192861</td>\n",
       "      <td>0.283486</td>\n",
       "      <td>0.330218</td>\n",
       "      <td>0.835221</td>\n",
       "      <td>0.879208</td>\n",
       "      <td>0.561135</td>\n",
       "      <td>0.276077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090393</td>\n",
       "      <td>0.077717</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.026802</td>\n",
       "      <td>0.023761</td>\n",
       "      <td>0.015058</td>\n",
       "      <td>0.175243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.140061</td>\n",
       "      <td>0.310871</td>\n",
       "      <td>0.113458</td>\n",
       "      <td>0.152146</td>\n",
       "      <td>0.075315</td>\n",
       "      <td>0.219089</td>\n",
       "      <td>0.105393</td>\n",
       "      <td>0.077829</td>\n",
       "      <td>0.150686</td>\n",
       "      <td>0.184609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286745</td>\n",
       "      <td>0.267726</td>\n",
       "      <td>0.052584</td>\n",
       "      <td>0.056957</td>\n",
       "      <td>0.014310</td>\n",
       "      <td>0.022642</td>\n",
       "      <td>0.161504</td>\n",
       "      <td>0.152302</td>\n",
       "      <td>0.121785</td>\n",
       "      <td>0.232748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.475238</td>\n",
       "      <td>0.161111</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.077309</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>0.155403</td>\n",
       "      <td>0.779528</td>\n",
       "      <td>0.838583</td>\n",
       "      <td>0.468504</td>\n",
       "      <td>0.142758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.568784</td>\n",
       "      <td>0.352778</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>0.156049</td>\n",
       "      <td>0.262274</td>\n",
       "      <td>0.280596</td>\n",
       "      <td>0.858268</td>\n",
       "      <td>0.889764</td>\n",
       "      <td>0.562992</td>\n",
       "      <td>0.238394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.652326</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.274875</td>\n",
       "      <td>0.312661</td>\n",
       "      <td>0.467613</td>\n",
       "      <td>0.909449</td>\n",
       "      <td>0.933071</td>\n",
       "      <td>0.661417</td>\n",
       "      <td>0.355500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0              1              2              3   \\\n",
       "count  581005.000000  581005.000000  581005.000000  581005.000000   \n",
       "mean        0.550458       0.432380       0.213692       0.192861   \n",
       "std         0.140061       0.310871       0.113458       0.152146   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.475238       0.161111       0.136364       0.077309   \n",
       "50%         0.568784       0.352778       0.196970       0.156049   \n",
       "75%         0.652326       0.722222       0.272727       0.274875   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                  4              5              6              7   \\\n",
       "count  581005.000000  581005.000000  581005.000000  581005.000000   \n",
       "mean        0.283486       0.330218       0.835221       0.879208   \n",
       "std         0.075315       0.219089       0.105393       0.077829   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.232558       0.155403       0.779528       0.838583   \n",
       "50%         0.262274       0.280596       0.858268       0.889764   \n",
       "75%         0.312661       0.467613       0.909449       0.933071   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                  8              9       ...                   45  \\\n",
       "count  581005.000000  581005.000000      ...        581005.000000   \n",
       "mean        0.561135       0.276077      ...             0.090393   \n",
       "std         0.150686       0.184609      ...             0.286745   \n",
       "min         0.000000       0.000000      ...             0.000000   \n",
       "25%         0.468504       0.142758      ...             0.000000   \n",
       "50%         0.562992       0.238394      ...             0.000000   \n",
       "75%         0.661417       0.355500      ...             0.000000   \n",
       "max         1.000000       1.000000      ...             1.000000   \n",
       "\n",
       "                  46             47             48             49  \\\n",
       "count  581005.000000  581005.000000  581005.000000  581005.000000   \n",
       "mean        0.077717       0.002773       0.003255       0.000205   \n",
       "std         0.267726       0.052584       0.056957       0.014310   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                  50             51             52             53  \\\n",
       "count  581005.000000  581005.000000  581005.000000  581005.000000   \n",
       "mean        0.000513       0.026802       0.023761       0.015058   \n",
       "std         0.022642       0.161504       0.152302       0.121785   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                  54  \n",
       "count  581005.000000  \n",
       "mean        0.175243  \n",
       "std         0.232748  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.166667  \n",
       "75%         0.166667  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 55 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "\n",
    "class Bandit_SGD(SGD):\n",
    "    \"\"\"Stochastic gradient descent optimizer for contextual bandits.\n",
    "    Includes support for momentum,\n",
    "    learning rate decay, and Nesterov momentum.\n",
    "    # Arguments\n",
    "        n_arms: int >0. Number of arms.\n",
    "        explore: float [0., .5] Exploration parameter.\n",
    "        lr: float >= 0. Learning rate.\n",
    "        momentum: float >= 0. Parameter updates momentum.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        nesterov: boolean. Whether to apply Nesterov momentum.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms=2., explore=.1, **kwargs):\n",
    "        super(Bandit_SGD, self).__init__(**kwargs)\n",
    "        self.n_arms = K.variable(n_arms, name='n_arms')\n",
    "        self.explore = K.variable(explore, name='explore')\n",
    "    \n",
    "    def get_updates(self, params, constraints, loss):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = []\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * self.iterations))\n",
    "            self.updates .append(K.update_add(self.iterations, 1))\n",
    "        \n",
    "        # weight scaling for bandits\n",
    "        P = (1. - self.explore) + self.explore / self.n_arms\n",
    "        # momentum\n",
    "        shapes = [K.get_variable_shape(p) for p in params]\n",
    "        moments = [K.zeros(shape) for shape in shapes]\n",
    "        self.weights = [self.iterations] + moments\n",
    "        for p, g, m in zip(params, grads, moments):\n",
    "            # apply bandit scaling\n",
    "            g =  g/P\n",
    "            v = self.momentum * m - lr * g  # velocity\n",
    "            self.updates.append(K.update(m, v))\n",
    "\n",
    "            if self.nesterov:\n",
    "                new_p = (p + self.momentum * v - lr * g)\n",
    "            else:\n",
    "                new_p = (p + v)\n",
    "\n",
    "            # apply constraints\n",
    "            if p in constraints:\n",
    "                c = constraints[p]\n",
    "                new_p = c(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "    \n",
    "class Bandit_Adam(Adam):\n",
    "    \"\"\"Adam optimizer for contextual bandits\n",
    "    Default parameters follow those provided in the original paper.\n",
    "    # Arguments\n",
    "        n_arms: int >0. Number of arms.\n",
    "        explore: float [0., .5] Exploration parameter.\n",
    "        lr: float >= 0. Learning rate.\n",
    "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "        epsilon: float >= 0. Fuzz factor.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "    # References\n",
    "        - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_arms=2., explore=.1, **kwargs):\n",
    "        super(Bandit_Adam, self).__init__(**kwargs)\n",
    "        self.n_arms = K.variable(n_arms, name='n_arms')\n",
    "        self.explore = K.variable(explore, name='explore')\n",
    "        \n",
    "\n",
    "    def get_updates(self, params, constraints, loss):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "        t = self.iterations + 1\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        shapes = [K.get_variable_shape(p) for p in params]\n",
    "        ms = [K.zeros(shape) for shape in shapes]\n",
    "        vs = [K.zeros(shape) for shape in shapes]\n",
    "        self.weights = [self.iterations] + ms + vs\n",
    "        # weight scaling for bandits\n",
    "        P = (1. - self.explore)*(loss > -0.6931471805599453) + self.explore / self.n_arms\n",
    "        \n",
    "        for p, g, m, v in zip(params, grads, ms, vs):\n",
    "            # apply bandit scaling\n",
    "            g = g/P\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "\n",
    "            new_p = p_t\n",
    "            # apply constraints\n",
    "            if p in constraints:\n",
    "                c = constraints[p]\n",
    "                new_p = c(new_p)\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                1792      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,825\n",
      "Trainable params: 1,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# mlp factory\n",
    "\n",
    "def build_experts(n, input_shape, n_hidden, n_layers):\n",
    "    # builds a committee of experts\n",
    "    def build_expert():\n",
    "        model = Sequential()\n",
    "        # add hidden layers\n",
    "        for layer in range(n_layers):\n",
    "            model.add(Dense(n_hidden,\n",
    "                            kernel_initializer='glorot_uniform',\n",
    "                            activation='relu',\n",
    "                            input_dim=input_shape,\n",
    "                            kernel_regularizer=regularizers.l2(0.01)))\n",
    "        # output layer\n",
    "        model.add(Dense(1,\n",
    "                        kernel_initializer='glorot_normal',\n",
    "                        activation='sigmoid',\n",
    "                        kernel_regularizer=regularizers.l2(0.01)))\n",
    "        return model\n",
    "    experts = [build_expert() for i in range(n)]\n",
    "    return experts\n",
    "\n",
    "experts = build_experts(4, X.shape[1], 32, 1)\n",
    "experts[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.models.Sequential at 0x2591ec8ab38>,\n",
       " <keras.models.Sequential at 0x2591ecc2978>,\n",
       " <keras.models.Sequential at 0x2591ed05e10>,\n",
       " <keras.models.Sequential at 0x2591edca940>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compile_experts(experts, optimizer, loss, **kwargs):\n",
    "    # compiles a commitee of experts\n",
    "    n_arms = len(experts)\n",
    "    def compile_expert(expert, **kwargs):\n",
    "        expert.compile(optimizer=optimizer,\n",
    "                      loss=loss)\n",
    "        return expert\n",
    "    compiled_experts = [compile_expert(expert) for expert in experts]\n",
    "    return compiled_experts\n",
    "\n",
    "# test it out\n",
    "experts = compile_experts(experts, 'adam', 'binary_crossentropy', explore=.1)\n",
    "experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25916a730f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# chooses an arm as in Algorithm 1\n",
    "def choose_arm(x, experts, explore):\n",
    "    n_arms = len(experts)\n",
    "    # make predictions\n",
    "    preds = [expert.predict(x) for expert in experts]\n",
    "    # get best arm\n",
    "    arm_max = np.nanargmax(preds)\n",
    "    # create arm selection probabilities\n",
    "    P = [(1-explore)*(arm==arm_max) + explore/n_arms for arm in range(n_arms)]\n",
    "    # select an arm\n",
    "    chosen_arm = np.random.choice(np.arange(n_arms), p=P)\n",
    "    pred = preds[chosen_arm]\n",
    "    return chosen_arm, pred\n",
    "\n",
    "# quick test\n",
    "starting_arms = pd.value_counts([choose_arm(X[[np.random.choice(range(X.shape[0]))]], experts, explore=.5)[0] for i in range(10)])\n",
    "sns.barplot(starting_arms.index, starting_arms.values);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_bandit_1(X, Y, explore, exp_annealing_rate=1, min_explore=.005, **kwargs):\n",
    "    n, n_arms = Y.shape\n",
    "    input_shape = X.shape[1]\n",
    "    experts = build_experts(n_arms, input_shape, 32, 1)\n",
    "    experts = compile_experts(experts, **kwargs)\n",
    "    # trace for arm choices\n",
    "    chosen_arms = []\n",
    "    # trace for regrets\n",
    "    regrets = []\n",
    "    true_rewards = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    message_iteration = 10\n",
    "    print(f'Starting bandit\\n----------\\nN_arms: {n_arms}\\n----------\\n')\n",
    "    for i in range(n):\n",
    "        context = X[[i]]\n",
    "        chosen_arm, pred = choose_arm(context, experts, explore)\n",
    "        reward = Y[i, chosen_arm]\n",
    "        max_reward = np.max(Y[i])\n",
    "        max_arm = np.argmax(Y[i])\n",
    "        true_rewards.append(max_arm)\n",
    "        expert = experts[chosen_arm]\n",
    "        expert.fit(context, np.expand_dims(reward, axis=0), epochs=1, verbose=0)\n",
    "        experts[chosen_arm] = expert\n",
    "        chosen_arms.append(chosen_arm)\n",
    "        regret = max_reward - reward\n",
    "        regrets.append(regret)\n",
    "        if explore > min_explore:\n",
    "            explore *= exp_annealing_rate\n",
    "        if (i % message_iteration == 0) and (i > 0):\n",
    "            if message_iteration <= 1e4:\n",
    "                message_iteration *= 10\n",
    "            elapsed = time.time() - start_time\n",
    "            remaining = (n*elapsed/i - elapsed)/60\n",
    "            print(f'''Completed iteration: {i}\n",
    "            Elapsed time: {elapsed:.2f} seconds\n",
    "            Estimated time remaining: {remaining:.2f} minutes\n",
    "            --------------------''')\n",
    "    elapsed = (time.time() - start_time)/60\n",
    "    print(f'Finished in: {elapsed:.2f} minutes')\n",
    "    return experts, chosen_arms, true_rewards, regrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting bandit\n",
      "----------\n",
      "N_arms: 7\n",
      "----------\n",
      "\n",
      "Completed iteration: 10\n",
      "            Elapsed time: 2.83 seconds\n",
      "            Estimated time remaining: 0.43 minutes\n",
      "            --------------------\n",
      "Finished in: 0.08 minutes\n"
     ]
    }
   ],
   "source": [
    "# sample run\n",
    "n_points = 100\n",
    "\n",
    "fit_models_1, arm_hist_1, true_reward_hist_1, regret_hist_1 = run_bandit_1(X[:n_points], Y[:n_points], optimizer='adam', \n",
    "                                                                           loss='binary_crossentropy', explore=.005, exp_annealing_rate=1, clipnorm=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEYCAYAAABGJWFlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2595096a4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.cumsum(regret_hist_1))\n",
    "plt.title('cumulative regret')\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('regret');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    94\n",
       "3     5\n",
       "5     1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# true best arms\n",
    "pd.value_counts(true_reward_hist_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    36\n",
       "4    20\n",
       "1    16\n",
       "0    14\n",
       "2     8\n",
       "6     4\n",
       "5     2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chosen arms\n",
    "pd.value_counts(arm_hist_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzGs4NR6EpAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2595096a550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(pd.value_counts(arm_hist_1).index, pd.value_counts(arm_hist_1).values)\n",
    "plt.title('chosen arm distribution')\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('arm');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2595096a160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(pd.value_counts(true_reward_hist_1).index, pd.value_counts(true_reward_hist_1).values)\n",
    "plt.title('true reward distribution')\n",
    "plt.ylabel('reward')\n",
    "plt.xlabel('arm');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# neural bandit 2\n",
    "\n",
    "# get shapes and number of arms\n",
    "n, n_arms = Y.shape\n",
    "input_shape = X.shape[1]\n",
    "\n",
    "# init models\n",
    "# 32 hidden units, 1 hidden layer, explore = .005\n",
    "model_1 = build_experts(n_arms, input_shape, n_hidden=32, n_layers=1)\n",
    "model_1 = compile_experts(model_1, loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# 64 hidden units, 1 hidden layer, explore = .005\n",
    "model_2 = build_experts(n_arms, input_shape, n_hidden=64, n_layers=1)\n",
    "model_2 = compile_experts(model_2, loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# 128 hidden units, 1 hidden layer, explore = .005\n",
    "model_3 = build_experts(n_arms, input_shape, n_hidden=128, n_layers=1)\n",
    "model_3 = compile_experts(model_3, loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "\n",
    "# 64 hidden units, 2 hidden layers, explore = .005\n",
    "model_4 = build_experts(n_arms, input_shape, n_hidden=64, n_layers=2)\n",
    "model_4 = compile_experts(model_4, loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "\n",
    "# 64 hidden units, 2 hidden layers, explore = .005\n",
    "model_5 = build_experts(n_arms, input_shape, n_hidden=128, n_layers=2)\n",
    "model_5 = compile_experts(model_5, loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "\n",
    "# 32 hidden units, 1 hidden layer, annealing_explore\n",
    "model_6 = build_experts(n_arms, input_shape, n_hidden=32, n_layers=1)\n",
    "model_6 = compile_experts(model_6, loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# 64 hidden units, 1 hidden layer, annealing_explore\n",
    "model_7 = build_experts(n_arms, input_shape, n_hidden=64, n_layers=1)\n",
    "model_7 = compile_experts(model_7, loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# 128 hidden units, 1 hidden layer, annealing_explore\n",
    "model_8 = build_experts(n_arms, input_shape, n_hidden=128, n_layers=1)\n",
    "model_8 = compile_experts(model_8, loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "\n",
    "# 64 hidden units, 2 hidden layers, annealing_explore\n",
    "model_9 = build_experts(n_arms, input_shape, n_hidden=64, n_layers=2)\n",
    "model_9 = compile_experts(model_9, loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "\n",
    "# 64 hidden units, 2 hidden layers, annealing_explore\n",
    "model_10 = build_experts(n_arms, input_shape, n_hidden=128, n_layers=2)\n",
    "model_10 = compile_experts(model_10, loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 complete in 14.9386568069458 seconds.\n",
      "Step 2 complete in 16.71857786178589 seconds.\n",
      "Step 4 complete in 22.91658353805542 seconds.\n",
      "Step 8 complete in 38.36069464683533 seconds.\n",
      "Step 16 complete in 43.93818473815918 seconds.\n",
      "Step 32 complete in 44.08518862724304 seconds.\n",
      "Step 64 complete in 44.354464054107666 seconds.\n"
     ]
    }
   ],
   "source": [
    "# set n_steps and model exploration parameter\n",
    "n_steps = 100\n",
    "gamma_model=.1\n",
    "\n",
    "# collect models, only 4 in the interest of time\n",
    "models = [model_2, model_4, model_7, model_9]\n",
    "n_models = len(models)\n",
    "# init weight vector\n",
    "weights = np.ones(n_models)\n",
    "# init model explore parameters\n",
    "explores = np.array([.005]*2 + [.5]*2)\n",
    "anneal = np.array([False]*2 + [True]*2)\n",
    "annealing_rate = .99995\n",
    "min_explore = .005\n",
    "\n",
    "def get_model_probabilities(weights, gamma_model):\n",
    "    # get probabilites of choosing each model\n",
    "    p = np.array([(1-gamma_model)*weight/sum(weights) + gamma_model/n_models for weight in weights])\n",
    "    return p\n",
    "\n",
    "def choose_model(weights, gamma_model, model_probabilities):\n",
    "    # choose a model based on weights\n",
    "    n_models = len(weights)\n",
    "    model = np.random.choice(np.arange(n_models), p=model_probabilities)\n",
    "    return model\n",
    "\n",
    "# init histories\n",
    "arm_hist_2 = []\n",
    "model_hist_2 = []\n",
    "regret_hist_2 = []\n",
    "weight_hist_2 = []\n",
    "\n",
    "# init timing vars\n",
    "start_time = time.time()\n",
    "next_check = 1\n",
    "\n",
    "# train the models\n",
    "for step in range(n_steps):\n",
    "    # store weights\n",
    "    weight_hist_2.append(weights)\n",
    "    # get probs and choose a model\n",
    "    p = get_model_probabilities(weights, gamma_model)\n",
    "    chosen_model = choose_model(weights, gamma_model, p)\n",
    "    # store model choice\n",
    "    model_hist_2.append(chosen_model)\n",
    "    # get a random data point\n",
    "    i = np.random.randint(X.shape[0])\n",
    "    context = X[[i]]\n",
    "    # choose an arm\n",
    "    chosen_arm, pred = choose_arm(context, models[chosen_model], explores[chosen_model])\n",
    "    # store arm selection\n",
    "    arm_hist_2.append(chosen_arm)\n",
    "    # observe reward and max reward\n",
    "    reward = Y[i, chosen_arm]\n",
    "    max_reward = np.max(Y[i])\n",
    "    # calculate and store regret\n",
    "    regret = max_reward - reward\n",
    "    regret_hist_2.append(regret)\n",
    "    # update the chosen arm for each model\n",
    "    for m, model in enumerate(models):\n",
    "        expert = model[chosen_arm]\n",
    "        expert.fit(context, np.expand_dims(reward, axis=0), epochs=1, verbose=0)\n",
    "        model[chosen_arm] = expert\n",
    "        # anneal explore param if necessary\n",
    "        if (anneal[m]) and (explores[m] > min_explore):\n",
    "            explores[m] *= annealing_rate\n",
    "    # update weights\n",
    "    weights[chosen_model] = weights[chosen_model]*np.exp((gamma_model*reward/(p[chosen_model]*n_models)))\n",
    "    # print progress\n",
    "    if step == next_check:\n",
    "        elapsed = time.time()-start_time\n",
    "        print(f'Step {step} complete in {elapsed} seconds.')\n",
    "        next_check *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-48-7ea0e47e7e11>:99: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow version of Neural Bandit 1\n",
    "\n",
    "N_EXPERTS = 7\n",
    "N_FEATURES = X.shape[1]\n",
    "N_HIDDEN = 128\n",
    "MAX_STEPS = len(X)\n",
    "START_EXPLORE = np.array([.005])\n",
    "\n",
    "graph2 = tf.Graph()\n",
    "with graph2.as_default():\n",
    "    \n",
    "    # placeholders for inputs, rewards\n",
    "    context = tf.placeholder(tf.float32,\n",
    "                             shape=[None, N_FEATURES],\n",
    "                             name='context')\n",
    "    reward = tf.placeholder(tf.float32,\n",
    "                            shape=[None, N_EXPERTS],\n",
    "                            name='reward')\n",
    "    \n",
    "    # setting the exploration parameter and annealing rate\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "    start_explore = tf.constant(dtype=tf.float32,\n",
    "                          name='explore',\n",
    "                          shape=[1],\n",
    "                          value=START_EXPLORE)\n",
    "    min_explore = tf.constant([.005],\n",
    "                              dtype=tf.float32,\n",
    "                              name='min_explore')\n",
    "    \n",
    "    explore_anneal = tf.constant([.99995],\n",
    "                                 dtype=tf.float32,\n",
    "                                 name='explore_annealing_rate')\n",
    "    \n",
    "    explore = start_explore * tf.pow(explore_anneal, tf.to_float(global_step + 1))\n",
    "    \n",
    "    explore = tf.maximum(explore, min_explore)\n",
    "    \n",
    "    \n",
    "    # initializing regret\n",
    "    cum_regret = tf.Variable([0], dtype=tf.int32, name='regret', trainable=False)\n",
    "    \n",
    "    \n",
    "    def build_arm_network(activations, n_hidden):\n",
    "        W1 = tf.Variable(tf.truncated_normal([n_hidden, n_hidden], stddev=.1),\n",
    "                        dtype=tf.float32,\n",
    "                        name='W1',)\n",
    "        B1 = tf.Variable(tf.truncated_normal([n_hidden], stddev=.1),\n",
    "                        dtype=tf.float32,\n",
    "                        name='B1')\n",
    "        W2 = tf.Variable(tf.truncated_normal([n_hidden, 1], stddev=.1),\n",
    "                         dtype=tf.float32,\n",
    "                         name='W2')\n",
    "        B2 = tf.Variable(tf.truncated_normal([1], stddev=.1),\n",
    "                         dtype=tf.float32,\n",
    "                         name='B2')\n",
    "        h1 = tf.matmul(activations, W1) + B1\n",
    "        a1 = tf.nn.relu(h1)\n",
    "        h2 = tf.matmul(a1, W2) + B2\n",
    "        return h2\n",
    "    \n",
    "    def loss_fn(target, y_hat):\n",
    "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=target,\n",
    "                                                            logits=y_hat,\n",
    "                                                            name='xentropy')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "        return loss\n",
    "    \n",
    "    # dense layer variables\n",
    "    W_shared = tf.Variable(tf.truncated_normal([N_FEATURES, N_HIDDEN], stddev=.1),\n",
    "                           dtype=tf.float32,\n",
    "                           name='shared_weights',)\n",
    "\n",
    "    B_shared = tf.Variable(tf.truncated_normal([N_HIDDEN], stddev=.1),\n",
    "                           dtype=tf.float32,\n",
    "                           name='shared_biases')\n",
    "    \n",
    "    h_shared = tf.matmul(context, W_shared) + B_shared\n",
    "    a_shared = tf.nn.relu(h_shared)\n",
    "    \n",
    "    y_hats = []\n",
    "    for exp in range(N_EXPERTS):\n",
    "        \n",
    "        # arm networks\n",
    "        with tf.variable_scope('arm_'+str(exp)):\n",
    "            h = build_arm_network(a_shared, N_HIDDEN)\n",
    "            y_hat = tf.nn.sigmoid(h, name='y_hat')\n",
    "            y_hats.append(y_hat)\n",
    "            loss = loss_fn(tf.slice(reward, begin=[0, exp], size=[1, 1]), y_hat)\n",
    "\n",
    "            # track losses\n",
    "            tf.summary.scalar('loss', loss)\n",
    "        \n",
    "        \n",
    "    # aggregate predictions for epsilon greedy\n",
    "    y_hats = tf.squeeze(tf.concat(y_hats, axis=0))\n",
    "    \n",
    "    # epsilon greedy arm choice\n",
    "    best_arm = tf.arg_max(y_hats, dimension=0)\n",
    "    best_arm_probs = tf.expand_dims(tf.concat([explore, 1 - explore], axis=0), 0)\n",
    "    best_arm_draw = tf.squeeze(tf.multinomial(tf.log(best_arm_probs), 1))\n",
    "    \n",
    "    def best_arm_fn():\n",
    "        return best_arm\n",
    "    \n",
    "    def random_arm():\n",
    "        counts = tf.ones([1, N_EXPERTS])\n",
    "        logs = tf.log(counts)\n",
    "        return tf.squeeze(tf.multinomial(logs, 1))\n",
    "    \n",
    "    chosen_arm = tf.cond(tf.equal(best_arm_draw, tf.ones_like(best_arm_draw)), # condition\n",
    "                                  best_arm_fn, # if True\n",
    "                                  random_arm) # if False\n",
    "    \n",
    "    tf.summary.histogram('chosen_arms', chosen_arm)\n",
    "    \n",
    "    # track regret\n",
    "    true_best_arm = tf.arg_max(reward, dimension=1)\n",
    "    regret = tf.logical_not(tf.equal(chosen_arm, true_best_arm))\n",
    "    cum_regret = tf.assign(cum_regret, (tf.add(tf.to_int32(regret), cum_regret)))\n",
    "    tf.summary.scalar('cumulative_regret', tf.squeeze(cum_regret))\n",
    "    \n",
    "    \n",
    "    # create optimizers and training ops for each arm\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    update_ops.append(cum_regret)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        optimizers = [tf.train.AdamOptimizer() for _ in range(N_EXPERTS)]\n",
    "        \n",
    "        train_ops = []\n",
    "        for exp in range(N_EXPERTS):\n",
    "            with tf.variable_scope('arm'+str(exp)):\n",
    "                train_ops.append(optimizers[exp].minimize(loss, global_step))\n",
    "    \n",
    "    def train_op(arm):\n",
    "        return train_ops[arm]\n",
    "    \n",
    "    # tensorboard logs\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(graph=graph2, logdir='./neural_bandit_1')\n",
    "\n",
    "    \n",
    "    init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at step 0, regret was [0] and explore was [ 0.005]\n",
      "at step 100, regret was [5] and explore was [ 0.005]\n",
      "at step 200, regret was [10] and explore was [ 0.005]\n",
      "at step 300, regret was [13] and explore was [ 0.005]\n",
      "at step 400, regret was [17] and explore was [ 0.005]\n",
      "at step 500, regret was [18] and explore was [ 0.005]\n",
      "at step 600, regret was [22] and explore was [ 0.005]\n",
      "at step 700, regret was [27] and explore was [ 0.005]\n",
      "at step 800, regret was [34] and explore was [ 0.005]\n",
      "at step 900, regret was [39] and explore was [ 0.005]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph2) as sess:\n",
    "    sess.run(init_op)\n",
    "    for step in range(1000):\n",
    "        i = np.random.randint(X.shape[0])\n",
    "        feed_dict={context: X[[i]], reward: Y[[i]]}\n",
    "        chosen_arm_, y_hats_= sess.run([chosen_arm, y_hats], feed_dict=feed_dict)\n",
    "        if step % 100 == 0:\n",
    "            summary, _, regret_, explore_ = sess.run([merged, train_op(chosen_arm_), cum_regret, explore], feed_dict=feed_dict)\n",
    "            writer.add_summary(summary, step)\n",
    "            print('at step {}, regret was {} and explore was {}'.format(step, regret_, explore_))\n",
    "        else:\n",
    "            sess.run([train_op(chosen_arm_)], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
