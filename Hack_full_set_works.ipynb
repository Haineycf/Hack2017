{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Neural Network: Creative Offer Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Hillary Martin, John Winther, Dawn Moyer & Christine Hainey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import functools\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\n",
    "\"CONVERTED\", \"C_1\",\"C_2\",\"C_3\",\"C_4\",\"C_5\",\"ST_1\",\"ST_2\",\"ST_3\",     \n",
    "\"ST_4\",\"ST_5\",\"ST_6\",\"I_1\",\"I_2\",\"I_3\",\"I_4\",\"I_5\",\"I_6\",     \n",
    "\"I_7\",\"I_8\",\"I_9\",\"A_0\",\"A_1\",\"A_2\",\"A_3\",\"A_4\",\"B_0\",     \n",
    "\"B_1\",\"B_2\",\"B_3\",\"B_4\",\"B_5\",\"B_6\" \n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gzip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import shutil\n",
    "#with gzip.open('file.txt.gz', 'rb') as f_in, open('file.txt', 'wb') as f_out:\n",
    "#    shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\chris'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('hack_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[columns] = df[columns].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        CONVERTED  C_1  C_2  C_3  C_4  C_5  ST_1  ST_2  ST_3  ST_4 ...   A_2  \\\n",
       "0              0    0    0    0    0    0     0     0     0     0 ...     0   \n",
       "1              0    0    0    0    0    0     0     0     0     0 ...     1   \n",
       "2              0    0    0    0    0    0     0     0     0     1 ...     0   \n",
       "3              0    0    0    0    0    0     0     0     0     1 ...     0   \n",
       "4              0    0    0    0    0    0     0     0     0     0 ...     0   \n",
       "5              0    0    0    0    0    0     1     0     0     0 ...     1   \n",
       "6              0    0    0    0    0    0     0     0     1     0 ...     1   \n",
       "7              0    0    0    0    0    0     1     0     0     0 ...     1   \n",
       "8              1    0    1    0    0    0     0     0     1     0 ...     1   \n",
       "9              0    0    0    0    0    0     0     0     0     0 ...     0   \n",
       "10             0    0    0    0    0    0     0     0     0     0 ...     1   \n",
       "11             0    0    0    0    0    0     0     0     0     0 ...     0   \n",
       "12             0    0    0    0    0    0     0     0     0     1 ...     0   \n",
       "13             0    0    0    0    0    0     0     0     0     1 ...     0   \n",
       "14             0    0    0    0    0    0     0     1     0     0 ...     0   \n",
       "15             0    0    0    0    0    0     0     1     0     0 ...     0   \n",
       "16             0    0    0    0    0    0     0     0     0     1 ...     0   \n",
       "17             0    0    0    0    0    0     0     0     0     0 ...     1   \n",
       "18             1    0    0    0    0    1     1     0     0     0 ...     0   \n",
       "19             0    0    0    0    0    0     0     0     0     0 ...     0   \n",
       "20             1    0    0    0    1    0     0     0     0     0 ...     0   \n",
       "21             0    0    0    0    0    0     1     0     0     0 ...     0   \n",
       "22             1    0    0    0    0    1     0     0     0     1 ...     0   \n",
       "23             0    0    0    0    0    0     1     0     0     0 ...     0   \n",
       "24             0    0    0    0    0    0     0     0     0     0 ...     0   \n",
       "25             0    0    0    0    0    0     0     0     0     1 ...     0   \n",
       "26             0    0    0    0    0    0     0     0     0     0 ...     0   \n",
       "27             0    0    0    0    0    0     1     0     0     0 ...     1   \n",
       "28             0    0    0    0    0    0     0     0     0     0 ...     0   \n",
       "29             0    0    0    0    0    0     0     0     1     0 ...     1   \n",
       "...          ...  ...  ...  ...  ...  ...   ...   ...   ...   ... ...   ...   \n",
       "54833          0    0    0    0    0    0     0     0     0     1 ...     0   \n",
       "54834          0    0    0    0    0    0     0     0     0     1 ...     0   \n",
       "54835          1    0    0    0    1    0     0     0     0     1 ...     0   \n",
       "54836          0    0    0    0    0    0     0     0     0     1 ...     1   \n",
       "54837          0    0    0    0    0    0     0     0     0     1 ...     0   \n",
       "54838          0    0    0    0    0    0     0     0     0     1 ...     1   \n",
       "54839          0    0    0    0    0    0     0     0     0     1 ...     1   \n",
       "54840          0    0    0    0    0    0     0     0     0     1 ...     0   \n",
       "54841          0    0    0    0    0    0     0     0     0     1 ...     0   \n",
       "54842          0    0    0    0    0    0     0     0     0     1 ...     0   \n",
       "54843          0    0    0    0    0    0     0     0     0     1 ...     0   \n",
       "54844          0    0    0    0    0    0     0     0     0     1 ...     0   \n",
       "54845          0    0    0    0    0    0     0     0     0     1 ...     1   \n",
       "54846          1    0    1    0    0    0     0     0     0     1 ...     1   \n",
       "54847          0    0    0    0    0    0     0     0     0     1 ...     0   \n",
       "54848          0    0    0    0    0    0     0     0     0     1 ...     0   \n",
       "54849          0    0    0    0    0    0     0     0     0     1 ...     1   \n",
       "54850          0    0    0    0    0    0     0     0     0     1 ...     1   \n",
       "54851          0    0    0    0    0    0     0     0     0     1 ...     1   \n",
       "54852          0    0    0    0    0    0     0     0     0     1 ...     0   \n",
       "54853          0    0    0    0    0    0     0     0     0     1 ...     1   \n",
       "54854          0    0    0    0    0    0     0     0     0     1 ...     0   \n",
       "54855          0    0    0    0    0    0     1     0     0     0 ...     0   \n",
       "54856          0    0    0    0    0    0     0     0     0     1 ...     1   \n",
       "54857          0    0    0    0    0    0     0     0     0     1 ...     0   \n",
       "54858          1    0    0    0    1    0     0     0     0     1 ...     1   \n",
       "54859          0    0    0    0    0    0     0     0     0     1 ...     1   \n",
       "54860          1    0    0    0    1    0     0     0     0     1 ...     0   \n",
       "54861          0    0    0    0    0    0     0     0     0     0 ...     0   \n",
       "54862          0    0    0    0    0    0     0     0     0     1 ...     1   \n",
       "\n",
       "       A_3  A_4  B_0  B_1  B_2  B_3  B_4  B_5  B_6  \n",
       "0        0    0    0    1    0    0    0    0    0  \n",
       "1        0    0    0    1    0    0    0    0    0  \n",
       "2        0    1    0    0    0    0    0    1    0  \n",
       "3        0    1    0    0    0    0    1    0    0  \n",
       "4        1    0    1    0    0    0    0    0    0  \n",
       "5        0    0    0    1    0    0    0    0    0  \n",
       "6        0    0    0    1    0    0    0    0    0  \n",
       "7        0    0    0    0    0    0    0    0    0  \n",
       "8        0    0    1    0    0    0    0    0    0  \n",
       "9        0    1    0    0    1    0    0    0    0  \n",
       "10       0    0    0    1    0    0    0    0    0  \n",
       "11       1    0    0    0    0    0    1    0    0  \n",
       "12       0    0    0    0    1    0    0    0    0  \n",
       "13       0    1    0    0    0    0    0    0    1  \n",
       "14       1    0    0    0    0    0    1    0    0  \n",
       "15       0    1    0    0    0    0    0    1    0  \n",
       "16       1    0    1    0    0    0    0    0    0  \n",
       "17       0    0    0    0    0    0    0    0    0  \n",
       "18       0    0    0    1    0    0    0    0    0  \n",
       "19       0    0    0    1    0    0    0    0    0  \n",
       "20       0    0    0    1    0    0    0    0    0  \n",
       "21       0    1    0    1    0    0    0    0    0  \n",
       "22       0    1    0    0    1    0    0    0    0  \n",
       "23       0    0    0    0    0    0    0    0    0  \n",
       "24       0    1    0    0    0    0    0    1    0  \n",
       "25       1    0    0    0    0    1    0    0    0  \n",
       "26       1    0    0    0    0    0    0    1    0  \n",
       "27       0    0    0    0    0    0    1    0    0  \n",
       "28       0    1    0    0    0    1    0    0    0  \n",
       "29       0    0    1    0    0    0    0    0    0  \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "54833    0    1    0    1    0    0    0    0    0  \n",
       "54834    0    1    0    0    0    1    0    0    0  \n",
       "54835    1    0    0    0    0    0    0    0    1  \n",
       "54836    0    0    0    1    0    0    0    0    0  \n",
       "54837    1    0    0    0    1    0    0    0    0  \n",
       "54838    0    0    0    0    0    1    0    0    0  \n",
       "54839    0    0    0    0    1    0    0    0    0  \n",
       "54840    0    1    0    0    0    0    1    0    0  \n",
       "54841    1    0    0    0    1    0    0    0    0  \n",
       "54842    0    0    0    1    0    0    0    0    0  \n",
       "54843    0    0    0    1    0    0    0    0    0  \n",
       "54844    1    0    0    0    0    0    0    0    0  \n",
       "54845    0    0    0    0    0    0    0    1    0  \n",
       "54846    0    0    0    0    0    0    0    0    0  \n",
       "54847    1    0    0    0    0    0    0    0    0  \n",
       "54848    1    0    0    0    0    0    0    1    0  \n",
       "54849    0    0    0    0    1    0    0    0    0  \n",
       "54850    0    0    0    0    1    0    0    0    0  \n",
       "54851    0    0    0    0    0    0    0    0    0  \n",
       "54852    1    0    0    0    1    0    0    0    0  \n",
       "54853    0    0    0    0    0    1    0    0    0  \n",
       "54854    0    1    0    0    0    0    0    1    0  \n",
       "54855    0    1    0    0    0    0    0    0    0  \n",
       "54856    0    0    0    0    0    0    0    1    0  \n",
       "54857    1    0    0    0    1    0    0    0    0  \n",
       "54858    0    0    0    0    0    1    0    0    0  \n",
       "54859    0    0    0    1    0    0    0    0    0  \n",
       "54860    0    1    0    0    1    0    0    0    0  \n",
       "54861    0    1    0    0    1    0    0    0    0  \n",
       "54862    0    0    0    0    1    0    0    0    0  \n",
       "\n",
       "[54863 rows x 33 columns]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONVERTED</th>\n",
       "      <th>C_1</th>\n",
       "      <th>C_2</th>\n",
       "      <th>C_3</th>\n",
       "      <th>C_4</th>\n",
       "      <th>C_5</th>\n",
       "      <th>ST_1</th>\n",
       "      <th>ST_2</th>\n",
       "      <th>ST_3</th>\n",
       "      <th>ST_4</th>\n",
       "      <th>...</th>\n",
       "      <th>A_2</th>\n",
       "      <th>A_3</th>\n",
       "      <th>A_4</th>\n",
       "      <th>B_0</th>\n",
       "      <th>B_1</th>\n",
       "      <th>B_2</th>\n",
       "      <th>B_3</th>\n",
       "      <th>B_4</th>\n",
       "      <th>B_5</th>\n",
       "      <th>B_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.149573</td>\n",
       "      <td>0.029692</td>\n",
       "      <td>0.029164</td>\n",
       "      <td>0.029856</td>\n",
       "      <td>0.030002</td>\n",
       "      <td>0.030859</td>\n",
       "      <td>0.129541</td>\n",
       "      <td>0.058072</td>\n",
       "      <td>0.064433</td>\n",
       "      <td>0.454077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450358</td>\n",
       "      <td>0.259319</td>\n",
       "      <td>0.148716</td>\n",
       "      <td>0.041813</td>\n",
       "      <td>0.202541</td>\n",
       "      <td>0.195523</td>\n",
       "      <td>0.110074</td>\n",
       "      <td>0.066876</td>\n",
       "      <td>0.153327</td>\n",
       "      <td>0.059421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.356655</td>\n",
       "      <td>0.169738</td>\n",
       "      <td>0.168266</td>\n",
       "      <td>0.170192</td>\n",
       "      <td>0.170594</td>\n",
       "      <td>0.172936</td>\n",
       "      <td>0.335801</td>\n",
       "      <td>0.233882</td>\n",
       "      <td>0.245525</td>\n",
       "      <td>0.497891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.497534</td>\n",
       "      <td>0.438265</td>\n",
       "      <td>0.355811</td>\n",
       "      <td>0.200164</td>\n",
       "      <td>0.401897</td>\n",
       "      <td>0.396607</td>\n",
       "      <td>0.312985</td>\n",
       "      <td>0.249809</td>\n",
       "      <td>0.360306</td>\n",
       "      <td>0.236413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          CONVERTED           C_1           C_2           C_3           C_4  \\\n",
       "count  54863.000000  54863.000000  54863.000000  54863.000000  54863.000000   \n",
       "mean       0.149573      0.029692      0.029164      0.029856      0.030002   \n",
       "std        0.356655      0.169738      0.168266      0.170192      0.170594   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                C_5          ST_1          ST_2          ST_3          ST_4  \\\n",
       "count  54863.000000  54863.000000  54863.000000  54863.000000  54863.000000   \n",
       "mean       0.030859      0.129541      0.058072      0.064433      0.454077   \n",
       "std        0.172936      0.335801      0.233882      0.245525      0.497891   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      1.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "           ...                A_2           A_3           A_4           B_0  \\\n",
       "count      ...       54863.000000  54863.000000  54863.000000  54863.000000   \n",
       "mean       ...           0.450358      0.259319      0.148716      0.041813   \n",
       "std        ...           0.497534      0.438265      0.355811      0.200164   \n",
       "min        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "25%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "50%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "75%        ...           1.000000      1.000000      0.000000      0.000000   \n",
       "max        ...           1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                B_1           B_2           B_3           B_4           B_5  \\\n",
       "count  54863.000000  54863.000000  54863.000000  54863.000000  54863.000000   \n",
       "mean       0.202541      0.195523      0.110074      0.066876      0.153327   \n",
       "std        0.401897      0.396607      0.312985      0.249809      0.360306   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                B_6  \n",
       "count  54863.000000  \n",
       "mean       0.059421  \n",
       "std        0.236413  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 33 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get X and Y training and testing\n",
    "X = df.values[:, 6:]\n",
    "Y = df.values[:, 0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X.type  df.ID = pd.to_numeric(df.ID, errors='coerce'\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.14957257,  0.02969214,  0.02916355,  0.02985619,  0.030002  ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "      <td>54863.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.129541</td>\n",
       "      <td>0.058072</td>\n",
       "      <td>0.064433</td>\n",
       "      <td>0.454077</td>\n",
       "      <td>0.088639</td>\n",
       "      <td>0.205239</td>\n",
       "      <td>0.277418</td>\n",
       "      <td>0.092467</td>\n",
       "      <td>0.071943</td>\n",
       "      <td>0.060897</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450358</td>\n",
       "      <td>0.259319</td>\n",
       "      <td>0.148716</td>\n",
       "      <td>0.041813</td>\n",
       "      <td>0.202541</td>\n",
       "      <td>0.195523</td>\n",
       "      <td>0.110074</td>\n",
       "      <td>0.066876</td>\n",
       "      <td>0.153327</td>\n",
       "      <td>0.059421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.335801</td>\n",
       "      <td>0.233882</td>\n",
       "      <td>0.245525</td>\n",
       "      <td>0.497891</td>\n",
       "      <td>0.284225</td>\n",
       "      <td>0.403879</td>\n",
       "      <td>0.447729</td>\n",
       "      <td>0.289686</td>\n",
       "      <td>0.258396</td>\n",
       "      <td>0.239144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.497534</td>\n",
       "      <td>0.438265</td>\n",
       "      <td>0.355811</td>\n",
       "      <td>0.200164</td>\n",
       "      <td>0.401897</td>\n",
       "      <td>0.396607</td>\n",
       "      <td>0.312985</td>\n",
       "      <td>0.249809</td>\n",
       "      <td>0.360306</td>\n",
       "      <td>0.236413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1             2             3             4   \\\n",
       "count  54863.000000  54863.000000  54863.000000  54863.000000  54863.000000   \n",
       "mean       0.129541      0.058072      0.064433      0.454077      0.088639   \n",
       "std        0.335801      0.233882      0.245525      0.497891      0.284225   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      1.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 5             6             7             8             9   \\\n",
       "count  54863.000000  54863.000000  54863.000000  54863.000000  54863.000000   \n",
       "mean       0.205239      0.277418      0.092467      0.071943      0.060897   \n",
       "std        0.403879      0.447729      0.289686      0.258396      0.239144   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      1.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "           ...                 17            18            19            20  \\\n",
       "count      ...       54863.000000  54863.000000  54863.000000  54863.000000   \n",
       "mean       ...           0.450358      0.259319      0.148716      0.041813   \n",
       "std        ...           0.497534      0.438265      0.355811      0.200164   \n",
       "min        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "25%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "50%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "75%        ...           1.000000      1.000000      0.000000      0.000000   \n",
       "max        ...           1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 21            22            23            24            25  \\\n",
       "count  54863.000000  54863.000000  54863.000000  54863.000000  54863.000000   \n",
       "mean       0.202541      0.195523      0.110074      0.066876      0.153327   \n",
       "std        0.401897      0.396607      0.312985      0.249809      0.360306   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 26  \n",
       "count  54863.000000  \n",
       "mean       0.059421  \n",
       "std        0.236413  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 27 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "\n",
    "class Bandit_SGD(SGD):\n",
    "    \"\"\"Stochastic gradient descent optimizer for contextual bandits.\n",
    "    Includes support for momentum,\n",
    "    learning rate decay, and Nesterov momentum.\n",
    "    # Arguments\n",
    "        n_arms: int >0. Number of arms.\n",
    "        explore: float [0., .5] Exploration parameter.\n",
    "        lr: float >= 0. Learning rate.\n",
    "        momentum: float >= 0. Parameter updates momentum.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        nesterov: boolean. Whether to apply Nesterov momentum.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms=2., explore=.1, **kwargs):\n",
    "        super(Bandit_SGD, self).__init__(**kwargs)\n",
    "        self.n_arms = K.variable(n_arms, name='n_arms')\n",
    "        self.explore = K.variable(explore, name='explore')\n",
    "    \n",
    "    def get_updates(self, params, constraints, loss):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = []\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * self.iterations))\n",
    "            self.updates .append(K.update_add(self.iterations, 1))\n",
    "        \n",
    "        # weight scaling for bandits\n",
    "        P = (1. - self.explore) + self.explore / self.n_arms\n",
    "        # momentum\n",
    "        shapes = [K.get_variable_shape(p) for p in params]\n",
    "        moments = [K.zeros(shape) for shape in shapes]\n",
    "        self.weights = [self.iterations] + moments\n",
    "        for p, g, m in zip(params, grads, moments):\n",
    "            # apply bandit scaling\n",
    "            g =  g/P\n",
    "            v = self.momentum * m - lr * g  # velocity\n",
    "            self.updates.append(K.update(m, v))\n",
    "\n",
    "            if self.nesterov:\n",
    "                new_p = (p + self.momentum * v - lr * g)\n",
    "            else:\n",
    "                new_p = (p + v)\n",
    "\n",
    "            # apply constraints\n",
    "            if p in constraints:\n",
    "                c = constraints[p]\n",
    "                new_p = c(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "    \n",
    "class Bandit_Adam(Adam):\n",
    "    \"\"\"Adam optimizer for contextual bandits\n",
    "    Default parameters follow those provided in the original paper.\n",
    "    # Arguments\n",
    "        n_arms: int >0. Number of arms.\n",
    "        explore: float [0., .5] Exploration parameter.\n",
    "        lr: float >= 0. Learning rate.\n",
    "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "        epsilon: float >= 0. Fuzz factor.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "    # References\n",
    "        - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_arms=2., explore=.1, **kwargs):\n",
    "        super(Bandit_Adam, self).__init__(**kwargs)\n",
    "        self.n_arms = K.variable(n_arms, name='n_arms')\n",
    "        self.explore = K.variable(explore, name='explore')\n",
    "        \n",
    "\n",
    "    def get_updates(self, params, constraints, loss):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "        t = self.iterations + 1\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        shapes = [K.get_variable_shape(p) for p in params]\n",
    "        ms = [K.zeros(shape) for shape in shapes]\n",
    "        vs = [K.zeros(shape) for shape in shapes]\n",
    "        self.weights = [self.iterations] + ms + vs\n",
    "        # weight scaling for bandits\n",
    "        P = (1. - self.explore)*(loss > -0.6931471805599453) + self.explore / self.n_arms\n",
    "        \n",
    "        for p, g, m, v in zip(params, grads, ms, vs):\n",
    "            # apply bandit scaling\n",
    "            g = g/P\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "\n",
    "            new_p = p_t\n",
    "            # apply constraints\n",
    "            if p in constraints:\n",
    "                c = constraints[p]\n",
    "                new_p = c(new_p)\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                896       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 929\n",
      "Trainable params: 929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# mlp factory\n",
    "\n",
    "def build_experts(n, input_shape, n_hidden, n_layers):\n",
    "    # builds a committee of experts\n",
    "    def build_expert():\n",
    "        model = Sequential()\n",
    "        # add hidden layers\n",
    "        for layer in range(n_layers):\n",
    "            model.add(Dense(n_hidden,\n",
    "                            kernel_initializer='glorot_uniform',\n",
    "                            activation='relu',\n",
    "                            input_dim=input_shape,\n",
    "                            kernel_regularizer=regularizers.l2(0.01)))\n",
    "        # output layer\n",
    "        model.add(Dense(1,\n",
    "                        kernel_initializer='glorot_normal',\n",
    "                        activation='sigmoid',\n",
    "                        kernel_regularizer=regularizers.l2(0.01)))\n",
    "        return model\n",
    "    experts = [build_expert() for i in range(n)]\n",
    "    return experts\n",
    "\n",
    "experts = build_experts(4, X.shape[1], 32, 1) #training size\n",
    "experts[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.models.Sequential at 0x288cd922748>,\n",
       " <keras.models.Sequential at 0x288ce18cfd0>,\n",
       " <keras.models.Sequential at 0x288cf1b3518>,\n",
       " <keras.models.Sequential at 0x288ce1c6e10>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compile_experts(experts, optimizer, loss, **kwargs):\n",
    "    # compiles a commitee of experts\n",
    "    n_arms = len(experts)\n",
    "    def compile_expert(expert, **kwargs):\n",
    "        expert.compile(optimizer=optimizer,\n",
    "                      loss=loss)\n",
    "        return expert\n",
    "    compiled_experts = [compile_expert(expert) for expert in experts]\n",
    "    return compiled_experts\n",
    "\n",
    "# test it out\n",
    "experts = compile_experts(experts, 'adam', 'binary_crossentropy', explore=.1)\n",
    "experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x288cddeb0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# chooses an arm as in Algorithm 1\n",
    "def choose_arm(x, experts, explore):\n",
    "    n_arms = len(experts)\n",
    "    # make predictions\n",
    "    preds = [expert.predict(x) for expert in experts]\n",
    "    # get best arm\n",
    "    arm_max = np.nanargmax(preds)\n",
    "    # create arm selection probabilities\n",
    "    P = [(1-explore)*(arm==arm_max) + explore/n_arms for arm in range(n_arms)]\n",
    "    # select an arm\n",
    "    chosen_arm = np.random.choice(np.arange(n_arms), p=P)\n",
    "    pred = preds[chosen_arm]\n",
    "    return chosen_arm, pred\n",
    "\n",
    "# quick test\n",
    "starting_arms = pd.value_counts([choose_arm(X[[np.random.choice(range(X.shape[0]))]], experts, explore=.5)[0] for i in range(10)])\n",
    "sns.barplot(starting_arms.index, starting_arms.values);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bandit_1(X, Y, explore, exp_annealing_rate=1, min_explore=.005, **kwargs):\n",
    "    n, n_arms = Y.shape\n",
    "    input_shape = X.shape[1]\n",
    "    experts = build_experts(n_arms, input_shape, 32, 1)\n",
    "    experts = compile_experts(experts, **kwargs)\n",
    "    # trace for arm choices\n",
    "    chosen_arms = []\n",
    "    # trace for regrets\n",
    "    regrets = []\n",
    "    true_rewards = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    message_iteration = 10\n",
    "    print(f'Starting bandit\\n----------\\nN_arms: {n_arms}\\n----------\\n')\n",
    "    for i in range(n):\n",
    "        context = X[[i]]\n",
    "        chosen_arm, pred = choose_arm(context, experts, explore)\n",
    "        reward = Y[i, chosen_arm]\n",
    "        max_reward = np.max(Y[i])\n",
    "        max_arm = np.argmax(Y[i])\n",
    "        true_rewards.append(max_arm)\n",
    "        expert = experts[chosen_arm]\n",
    "        expert.fit(context, np.expand_dims(reward, axis=0), epochs=1, verbose=0)\n",
    "        experts[chosen_arm] = expert\n",
    "        chosen_arms.append(chosen_arm)\n",
    "        regret = max_reward - reward\n",
    "        regrets.append(regret)\n",
    "        if explore > min_explore:\n",
    "            explore *= exp_annealing_rate\n",
    "        if (i % message_iteration == 0) and (i > 0):\n",
    "            if message_iteration <= 1e4:\n",
    "                message_iteration *= 10\n",
    "            elapsed = time.time() - start_time\n",
    "            remaining = (n*elapsed/i - elapsed)/60\n",
    "            print(f'''Completed iteration: {i}\n",
    "            Elapsed time: {elapsed:.2f} seconds\n",
    "            Estimated time remaining: {remaining:.2f} minutes\n",
    "            --------------------''')\n",
    "    elapsed = (time.time() - start_time)/60\n",
    "    print(f'Finished in: {elapsed:.2f} minutes')\n",
    "    return experts, chosen_arms, true_rewards, regrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting bandit\n",
      "----------\n",
      "N_arms: 5\n",
      "----------\n",
      "\n",
      "Completed iteration: 10\n",
      "            Elapsed time: 3.91 seconds\n",
      "            Estimated time remaining: 0.59 minutes\n",
      "            --------------------\n",
      "Finished in: 0.19 minutes\n"
     ]
    }
   ],
   "source": [
    "# sample run\n",
    "n_points = 100  ## was 10 too low\n",
    "\n",
    "fit_models_1, arm_hist_1, true_reward_hist_1, regret_hist_1 = run_bandit_1(X[:n_points], Y[:n_points], optimizer='adam', \n",
    "                                                                           loss='binary_crossentropy', explore=.005, exp_annealing_rate=1, clipnorm=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x288d2ff3588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.cumsum(regret_hist_1))\n",
    "plt.title('cumulative regret')\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('regret');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    100\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# true best arms\n",
    "pd.value_counts(true_reward_hist_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    41\n",
       "3    33\n",
       "2    12\n",
       "4     7\n",
       "0     7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chosen arms\n",
    "pd.value_counts(arm_hist_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x288d3e87f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(pd.value_counts(arm_hist_1).index, pd.value_counts(arm_hist_1).values)\n",
    "plt.title('chosen arm distribution')\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('arm');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x288d302ff28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(pd.value_counts(true_reward_hist_1).index, pd.value_counts(true_reward_hist_1).values)\n",
    "plt.title('true reward distribution')\n",
    "plt.ylabel('reward')\n",
    "plt.xlabel('arm');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural bandit 2\n",
    "\n",
    "# get shapes and number of arms\n",
    "n, n_arms = Y.shape\n",
    "input_shape = X.shape[1]\n",
    "\n",
    "# init models\n",
    "# 32 hidden units, 1 hidden layer, explore = .005\n",
    "model_1 = build_experts(n_arms, input_shape, n_hidden=32, n_layers=1)\n",
    "model_1 = compile_experts(model_1, loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# 64 hidden units, 1 hidden layer, explore = .005\n",
    "model_2 = build_experts(n_arms, input_shape, n_hidden=64, n_layers=1)\n",
    "model_2 = compile_experts(model_2, loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# 128 hidden units, 1 hidden layer, explore = .005\n",
    "model_3 = build_experts(n_arms, input_shape, n_hidden=128, n_layers=1)\n",
    "model_3 = compile_experts(model_3, loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "\n",
    "# 64 hidden units, 2 hidden layers, explore = .005\n",
    "model_4 = build_experts(n_arms, input_shape, n_hidden=64, n_layers=2)\n",
    "model_4 = compile_experts(model_4, loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "\n",
    "# 64 hidden units, 2 hidden layers, explore = .005\n",
    "model_5 = build_experts(n_arms, input_shape, n_hidden=128, n_layers=2)\n",
    "model_5 = compile_experts(model_5, loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "\n",
    "# 32 hidden units, 1 hidden layer, annealing_explore\n",
    "model_6 = build_experts(n_arms, input_shape, n_hidden=32, n_layers=1)\n",
    "model_6 = compile_experts(model_6, loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# 64 hidden units, 1 hidden layer, annealing_explore\n",
    "model_7 = build_experts(n_arms, input_shape, n_hidden=64, n_layers=1)\n",
    "model_7 = compile_experts(model_7, loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# 128 hidden units, 1 hidden layer, annealing_explore\n",
    "model_8 = build_experts(n_arms, input_shape, n_hidden=128, n_layers=1)\n",
    "model_8 = compile_experts(model_8, loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "\n",
    "# 64 hidden units, 2 hidden layers, annealing_explore\n",
    "model_9 = build_experts(n_arms, input_shape, n_hidden=64, n_layers=2)\n",
    "model_9 = compile_experts(model_9, loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "\n",
    "# 64 hidden units, 2 hidden layers, annealing_explore\n",
    "model_10 = build_experts(n_arms, input_shape, n_hidden=128, n_layers=2)\n",
    "model_10 = compile_experts(model_10, loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 complete in 7.085283041000366 seconds.\n",
      "Step 2 complete in 7.779978513717651 seconds.\n",
      "Step 4 complete in 11.368881702423096 seconds.\n",
      "Step 8 complete in 18.004453897476196 seconds.\n",
      "Step 16 complete in 22.194936752319336 seconds.\n",
      "Step 32 complete in 29.141674757003784 seconds.\n",
      "Step 64 complete in 42.30711054801941 seconds.\n"
     ]
    }
   ],
   "source": [
    "# set n_steps and model exploration parameter\n",
    "n_steps = 100\n",
    "gamma_model=.1\n",
    "\n",
    "# collect models, only 4 in the interest of time\n",
    "models = [model_2, model_4, model_7, model_9]\n",
    "n_models = len(models)\n",
    "# init weight vector\n",
    "weights = np.ones(n_models)\n",
    "# init model explore parameters\n",
    "explores = np.array([.005]*2 + [.5]*2)\n",
    "anneal = np.array([False]*2 + [True]*2)\n",
    "annealing_rate = .99995\n",
    "min_explore = .005\n",
    "\n",
    "def get_model_probabilities(weights, gamma_model):\n",
    "    # get probabilites of choosing each model\n",
    "    p = np.array([(1-gamma_model)*weight/sum(weights) + gamma_model/n_models for weight in weights])\n",
    "    return p\n",
    "\n",
    "def choose_model(weights, gamma_model, model_probabilities):\n",
    "    # choose a model based on weights\n",
    "    n_models = len(weights)\n",
    "    model = np.random.choice(np.arange(n_models), p=model_probabilities)\n",
    "    return model\n",
    "\n",
    "# init histories\n",
    "arm_hist_2 = []\n",
    "model_hist_2 = []\n",
    "regret_hist_2 = []\n",
    "weight_hist_2 = []\n",
    "\n",
    "# init timing vars\n",
    "start_time = time.time()\n",
    "next_check = 1\n",
    "\n",
    "# train the models\n",
    "for step in range(n_steps):\n",
    "    # store weights\n",
    "    weight_hist_2.append(weights)\n",
    "    # get probs and choose a model\n",
    "    p = get_model_probabilities(weights, gamma_model)\n",
    "    chosen_model = choose_model(weights, gamma_model, p)\n",
    "    # store model choice\n",
    "    model_hist_2.append(chosen_model)\n",
    "    # get a random data point\n",
    "    i = np.random.randint(X.shape[0])\n",
    "    context = X[[i]]\n",
    "    # choose an arm\n",
    "    chosen_arm, pred = choose_arm(context, models[chosen_model], explores[chosen_model])\n",
    "    # store arm selection\n",
    "    arm_hist_2.append(chosen_arm)\n",
    "    # observe reward and max reward\n",
    "    reward = Y[i, chosen_arm]\n",
    "    max_reward = np.max(Y[i])\n",
    "    # calculate and store regret\n",
    "    regret = max_reward - reward\n",
    "    regret_hist_2.append(regret)\n",
    "    # update the chosen arm for each model\n",
    "    for m, model in enumerate(models):\n",
    "        expert = model[chosen_arm]\n",
    "        expert.fit(context, np.expand_dims(reward, axis=0), epochs=1, verbose=0)\n",
    "        model[chosen_arm] = expert\n",
    "        # anneal explore param if necessary\n",
    "        if (anneal[m]) and (explores[m] > min_explore):\n",
    "            explores[m] *= annealing_rate\n",
    "    # update weights\n",
    "    weights[chosen_model] = weights[chosen_model]*np.exp((gamma_model*reward/(p[chosen_model]*n_models)))\n",
    "    # print progress\n",
    "    if step == next_check:\n",
    "        elapsed = time.time()-start_time\n",
    "        print(f'Step {step} complete in {elapsed} seconds.')\n",
    "        next_check *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow version of Neural Bandit 1\n",
    "\n",
    "N_EXPERTS = 5\n",
    "N_FEATURES = X.shape[1]\n",
    "N_HIDDEN = 100\n",
    "MAX_STEPS = len(X)\n",
    "START_EXPLORE = np.array([.005])\n",
    "\n",
    "graph2 = tf.Graph()\n",
    "with graph2.as_default():\n",
    "    \n",
    "    # placeholders for inputs, rewards\n",
    "    context = tf.placeholder(tf.float32,\n",
    "                             shape=[None, N_FEATURES],\n",
    "                             name='context')\n",
    "    reward = tf.placeholder(tf.float32,\n",
    "                            shape=[None, N_EXPERTS],\n",
    "                            name='reward')\n",
    "    \n",
    "    # setting the exploration parameter and annealing rate\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "    start_explore = tf.constant(dtype=tf.float32,\n",
    "                          name='explore',\n",
    "                          shape=[1],\n",
    "                          value=START_EXPLORE)\n",
    "    min_explore = tf.constant([.005],\n",
    "                              dtype=tf.float32,\n",
    "                              name='min_explore')\n",
    "    \n",
    "    explore_anneal = tf.constant([.99995],\n",
    "                                 dtype=tf.float32,\n",
    "                                 name='explore_annealing_rate')\n",
    "    \n",
    "    explore = start_explore * tf.pow(explore_anneal, tf.to_float(global_step + 1))\n",
    "    \n",
    "    explore = tf.maximum(explore, min_explore)\n",
    "    \n",
    "    \n",
    "    # initializing regret\n",
    "    cum_regret = tf.Variable([0], dtype=tf.int32, name='regret', trainable=False)\n",
    "    \n",
    "    \n",
    "    def build_arm_network(activations, n_hidden):\n",
    "        W1 = tf.Variable(tf.truncated_normal([n_hidden, n_hidden], stddev=.1),\n",
    "                        dtype=tf.float32,\n",
    "                        name='W1',)\n",
    "        B1 = tf.Variable(tf.truncated_normal([n_hidden], stddev=.1),\n",
    "                        dtype=tf.float32,\n",
    "                        name='B1')\n",
    "        W2 = tf.Variable(tf.truncated_normal([n_hidden, 1], stddev=.1),\n",
    "                         dtype=tf.float32,\n",
    "                         name='W2')\n",
    "        B2 = tf.Variable(tf.truncated_normal([1], stddev=.1),\n",
    "                         dtype=tf.float32,\n",
    "                         name='B2')\n",
    "        h1 = tf.matmul(activations, W1) + B1\n",
    "        a1 = tf.nn.relu(h1)\n",
    "        h2 = tf.matmul(a1, W2) + B2\n",
    "        return h2\n",
    "    \n",
    "    def loss_fn(target, y_hat):\n",
    "        cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=target,\n",
    "                                                            logits=y_hat,\n",
    "                                                            name='xentropy')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "        return loss\n",
    "    \n",
    "    # dense layer variables\n",
    "    W_shared = tf.Variable(tf.truncated_normal([N_FEATURES, N_HIDDEN], stddev=.1),\n",
    "                           dtype=tf.float32,\n",
    "                           name='shared_weights',)\n",
    "\n",
    "    B_shared = tf.Variable(tf.truncated_normal([N_HIDDEN], stddev=.1),\n",
    "                           dtype=tf.float32,\n",
    "                           name='shared_biases')\n",
    "    \n",
    "    h_shared = tf.matmul(context, W_shared) + B_shared\n",
    "    a_shared = tf.nn.relu(h_shared)\n",
    "    \n",
    "    y_hats = []\n",
    "    for exp in range(N_EXPERTS):\n",
    "        \n",
    "        # arm networks\n",
    "        with tf.variable_scope('arm_'+str(exp)):\n",
    "            h = build_arm_network(a_shared, N_HIDDEN)\n",
    "            y_hat = tf.nn.sigmoid(h, name='y_hat')\n",
    "            y_hats.append(y_hat)\n",
    "            loss = loss_fn(tf.slice(reward, begin=[0, exp], size=[1, 1]), y_hat)\n",
    "\n",
    "            # track losses\n",
    "            tf.summary.scalar('loss', loss)\n",
    "        \n",
    "        \n",
    "    # aggregate predictions for epsilon greedy\n",
    "    y_hats = tf.squeeze(tf.concat(y_hats, axis=0))\n",
    "    \n",
    "    # epsilon greedy arm choice\n",
    "    best_arm = tf.arg_max(y_hats, dimension=0)\n",
    "    best_arm_probs = tf.expand_dims(tf.concat([explore, 1 - explore], axis=0), 0)\n",
    "    best_arm_draw = tf.squeeze(tf.multinomial(tf.log(best_arm_probs), 1))\n",
    "    \n",
    "    def best_arm_fn():\n",
    "        return best_arm\n",
    "    \n",
    "    def random_arm():\n",
    "        counts = tf.ones([1, N_EXPERTS])\n",
    "        logs = tf.log(counts)\n",
    "        return tf.squeeze(tf.multinomial(logs, 1))\n",
    "    \n",
    "    chosen_arm = tf.cond(tf.equal(best_arm_draw, tf.ones_like(best_arm_draw)), # condition\n",
    "                                  best_arm_fn, # if True\n",
    "                                  random_arm) # if False\n",
    "    \n",
    "    tf.summary.histogram('chosen_arms', chosen_arm)\n",
    "    \n",
    "    # track regret\n",
    "    true_best_arm = tf.arg_max(reward, dimension=1)\n",
    "    regret = tf.logical_not(tf.equal(chosen_arm, true_best_arm))\n",
    "    cum_regret = tf.assign(cum_regret, (tf.add(tf.to_int32(regret), cum_regret)))\n",
    "    tf.summary.scalar('cumulative_regret', tf.squeeze(cum_regret))\n",
    "    \n",
    "    \n",
    "    # create optimizers and training ops for each arm\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    update_ops.append(cum_regret)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        optimizers = [tf.train.AdamOptimizer() for _ in range(N_EXPERTS)]\n",
    "        \n",
    "        train_ops = []\n",
    "        for exp in range(N_EXPERTS):\n",
    "            with tf.variable_scope('arm'+str(exp)):\n",
    "                train_ops.append(optimizers[exp].minimize(loss, global_step))\n",
    "    \n",
    "    def train_op(arm):\n",
    "        return train_ops[arm]\n",
    "    \n",
    "    # tensorboard logs\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(graph=graph2, logdir='./neural_bandit_1')\n",
    "\n",
    "    \n",
    "    init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at step 0, regret was [1] and explore was [ 0.005]\n",
      "at step 100, regret was [98] and explore was [ 0.005]\n",
      "at step 200, regret was [196] and explore was [ 0.005]\n",
      "at step 300, regret was [295] and explore was [ 0.005]\n",
      "at step 400, regret was [395] and explore was [ 0.005]\n",
      "at step 500, regret was [494] and explore was [ 0.005]\n",
      "at step 600, regret was [593] and explore was [ 0.005]\n",
      "at step 700, regret was [693] and explore was [ 0.005]\n",
      "at step 800, regret was [793] and explore was [ 0.005]\n",
      "at step 900, regret was [893] and explore was [ 0.005]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph2) as sess:\n",
    "    sess.run(init_op)\n",
    "    for step in range(1000):\n",
    "        i = np.random.randint(X.shape[0])\n",
    "        feed_dict={context: X[[i]], reward: Y[[i]]}\n",
    "        chosen_arm_, y_hats_= sess.run([chosen_arm, y_hats], feed_dict=feed_dict)\n",
    "        if step % 100 == 0:\n",
    "            summary, _, regret_, explore_ = sess.run([merged, train_op(chosen_arm_), cum_regret, explore], feed_dict=feed_dict)\n",
    "            writer.add_summary(summary, step)\n",
    "            print('at step {}, regret was {} and explore was {}'.format(step, regret_, explore_))\n",
    "        else:\n",
    "            sess.run([train_op(chosen_arm_)], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
